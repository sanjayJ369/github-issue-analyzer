# IssueInsight - Agentic GitHub Issue Analyzer

A powerful AI-powered tool that transforms messy GitHub issues into structured engineering insights. Built with **FastAPI** + **React** + **Google Gemini**.

[![Live Demo](https://img.shields.io/badge/Live%20Demo-Vercel-black?style=for-the-badge&logo=vercel)](https://github-issue-analyzer-smoky.vercel.app/)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-black?style=for-the-badge&logo=github)](https://github.com/sanjayJ369/github-issue-analyzer/)

## âœ¨ Features

- **Auto-Detection LLM Providers**: Dynamically detects API keys from `.env` and verifies availability
- **Latency Measurement**: Real response time measurement for each model with performance labels
- **Rate-Limit Handling**: Automatic fallback to next available provider on 429 errors
- **Structured Analysis**: Summary, classification, priority scoring, and label suggestions
- **Multi-Provider Support**: Gemini, OpenAI, Anthropic, and Hugging Face
- **Modern UI**: React-based SPA with dark/light theme, responsive design
- **Agentic UX**: Real-time status feed showing analysis pipeline progress

### Extra Features
- ğŸ“‹ Copy JSON button for easy export
- âš ï¸ Closed issue warnings
- ğŸ’¾ 15-minute TTL caching for fast repeat queries
- ğŸ”„ Intelligent error handling with retry options
- ğŸ”€ Provider selection with localStorage persistence
- â±ï¸ Latency-based speed indicators (Fast/Medium/Slow)

---

## ğŸŒ Live Demo

**Try it now**: [https://github-issue-analyzer-smoky.vercel.app/](https://github-issue-analyzer-smoky.vercel.app/)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React SPA     â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI       â”‚â”€â”€â”€â”€â–¶â”‚   GitHub API    â”‚
â”‚   (Vite + TW)   â”‚     â”‚   Backend       â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  LLM Providers  â”‚
                        â”‚  (Auto-Detect)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer | Technology | Purpose |
|-------|------------|---------|
| Frontend | React + Vite + Tailwind CSS | Modern, responsive UI |
| Backend | FastAPI + Pydantic | Type-safe API with validation |
| AI/LLM | Gemini, OpenAI, Anthropic, HuggingFace | Multi-provider support with auto-detection |
| Data | GitHub REST API + HTTPX | Async data fetching with retries |

---

## ğŸš€ Quick Start (< 5 Minutes)

### Prerequisites
- Python 3.11+
- Node.js 18+
- At least one LLM API Key:
  - [Google Gemini](https://aistudio.google.com/app/apikey) (free)
  - [OpenAI](https://platform.openai.com/api-keys)
  - [Anthropic](https://console.anthropic.com/)
  - [Hugging Face](https://huggingface.co/settings/tokens)
- (Optional) GitHub Token for higher rate limits

---

### ğŸ§ Linux / macOS Installation

```bash
# 1. Clone the repository
git clone https://github.com/sanjayJ369/github-issue-analyzer.git
cd github-issue-analyzer

# 2. Configure environment
cp .env.example .env
nano .env  # Add your API keys

# 3. Start Backend (Terminal 1)
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000

# 4. Start Frontend (Terminal 2)
cd frontend
npm install
npm run dev
```

---

### ğŸªŸ Windows Installation

```powershell
# 1. Clone the repository
git clone https://github.com/sanjayJ369/github-issue-analyzer.git
cd github-issue-analyzer

# 2. Configure environment
copy .env.example .env
# Open .env in notepad and add your API keys

# 3. Start Backend (Terminal 1 - PowerShell)
cd backend
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000

# 4. Start Frontend (Terminal 2 - PowerShell)
cd frontend
npm install
npm run dev
```

> **Note for Windows**: If you encounter execution policy errors, run:
> ```powershell
> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
> ```

---

### âœ… Verify Installation

1. Backend API: http://localhost:8000/docs (Swagger UI)
2. Frontend App: http://localhost:5173
3. Try analyzing: `https://github.com/fastapi/fastapi/issues/1`

---

## ğŸ”‘ Environment Variables

```bash
# LLM Providers (at least one required)
GEMINI_API_KEY=your_gemini_key
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
HF_API_KEY=your_huggingface_key

# Optional
GITHUB_TOKEN=your_github_token  # Higher rate limits
MODEL_NAME=gemini-2.0-flash     # Default model
```

The app will auto-detect which providers are available and verify them with real LLM calls.

---

## ğŸ“‚ Project Structure

```
github-issue-analyzer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ github_client.py  # GitHub API integration
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ providers.py  # Auto-detection & latency measurement
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py     # Provider routing with fallback
â”‚   â”‚   â”‚   â””â”€â”€ clients/      # Provider-specific clients
â”‚   â”‚   â”œâ”€â”€ schemas.py        # Pydantic models
â”‚   â”‚   â””â”€â”€ utils.py          # URL parsing, context building
â”‚   â”œâ”€â”€ tests/                # 11 test cases
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”œâ”€â”€ api.js            # API client
â”‚   â”‚   â””â”€â”€ App.jsx           # Main application
â”‚   â””â”€â”€ package.json
â””â”€â”€ vercel.json               # Vercel deployment config
```

---

## ğŸ§ª Testing

```bash
cd backend
pytest tests/ -v
```

**11 tests covering:**
- Health check endpoint
- Provider listing with latency
- Success/error scenarios (404, 403, 400, 422)
- Closed issue warnings
- Response structure validation

---

## ğŸŒ Deployment

### Vercel (Production)
- **Live URL**: [https://github-issue-analyzer-smoky.vercel.app/](https://github-issue-analyzer-smoky.vercel.app/)
- Frontend: Static build via `@vercel/static-build`
- Backend: Python serverless function via `@vercel/python`
- Environment variables: Set API keys in Vercel dashboard

### GitHub Actions CI
- Runs on every push/PR to `main`
- Executes full test suite
- See `.github/workflows/ci-cd.yml`

---

## ğŸ’¡ Key Features

### LLM Provider Auto-Detection
| Feature | Description |
|---------|-------------|
| ğŸ” **Auto-Detection** | Scans `.env` for API keys automatically |
| â±ï¸ **Latency Measurement** | Real response time for each model |
| ğŸ·ï¸ **Speed Labels** | Fast (<1s), Medium (1-3s), Slow (>3s) |
| ğŸ”„ **Rate-Limit Fallback** | Auto-retry with next available provider |
| ğŸ”’ **Concurrency Limiting** | Semaphore-based (max 5 parallel checks) |

### UI/UX Enhancements
| Feature | Description |
|---------|-------------|
| ğŸ“‹ **Copy JSON Button** | One-click export of analysis results |
| ğŸŒ“ **Theme Toggle** | Dark/Light mode with persistence |
| ğŸ“± **Responsive Design** | Adaptive layout for all devices |
| âœ¨ **Status Feed** | Real-time visualization of pipeline |
| ğŸ¨ **Modern UI** | Glassmorphism effects with animations |

---

## ğŸ“ License

MIT License - Feel free to use and modify!

---

## ğŸ”— Links

- **Live Demo**: [https://github-issue-analyzer-smoky.vercel.app/](https://github-issue-analyzer-smoky.vercel.app/)
- **GitHub**: [https://github.com/sanjayJ369/github-issue-analyzer/](https://github.com/sanjayJ369/github-issue-analyzer/)
